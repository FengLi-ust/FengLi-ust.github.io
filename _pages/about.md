---
permalink: /
title: About Me
excerpt: "Feng Li's Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a third-year PhD (2021-now) student at the [Department of Computer Science & Engineering](https://cse.hkust.edu.hk/), [Hong Kong University of Science and Technology](https://hkust.edu.hk/), co-supervised by Prof. [Heung-Yeung Shum](https://scholar.google.com/citations?user=9akH-n8AAAAJ&hl=zh-CN) and Prof. [Lionel M. Ni](https://scholar.google.com/citations?user=OzMYwDIAAAAJ&hl=zh-CN). Previously, I obtained my bachelor‚Äôs degree from Computer Science and Technology, South China University of Science and Technology in 2021. 

I am currently an intern at FAIR MPK (Facebook AI Research, Menlo Park). I have interned at [International Digital Economy Academy, Shenzhen](https://idea.edu.cn/) (advised by Prof. [Lei Zhang](https://www.leizhang.org/)) and [Microsoft Research, Redmond](https://www.microsoft.com/en-us/research/group/deep-learning-group/) (advised by Dr. [Jianwei Yang](https://jwyang.github.io/) and Dr. [Chunyuan Li](https://chunyuan.li/)). 


I anticipate graduating in 2025 and am open to both academic and industrial research positions in North America and Asia. If you are interested, please feel free to contact me.

<!-- I am always open to research discussions and collaborations. Feel free to get in touch! -->
<!-- 
**Research Interests** -->

üìåMy research interests lie in visual understanding/generation and multi-modal learning.

‚úâÔ∏è Welcome to contact me for any discussion and cooperation!


# üî• News
- \[2024/3\]: &nbsp;Check our recent works on **Visual Prompting** for detection and segmentation! A series of works including [DINOv](https://arxiv.org/pdf/2311.13601.pdf)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv?style=social">, [T-Rex/T-Rex2](https://arxiv.org/abs/2311.13596)![Github stars](https://img.shields.io/github/stars/IDEA-Research/T-Rex.svg) have been released.
- \[2023/9\]: &nbsp;[Mask DINO](https://arxiv.org/pdf/2206.02777.pdf) ranks **9th** among [the most influential CVPR 2023 papers](https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/).
- \[2023/7\]: &nbsp;Two works that focus on **Interactive Segmentation** have been released, including [SEEM](https://arxiv.org/pdf/2304.06718.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg) and [Semantic-SAM](https://arxiv.org/pdf/2307.04767.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Semantic-SAM.svg). Check them out!
- \[2023/4\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) ranks **2nd** among [the most influential ICLR 2023 papers](https://www.paperdigest.org/2023/04/most-influential-iclr-papers-2023-04/).
- \[2023/3\]: &nbsp;Following DINO, two works that focus on **Open-set Detection \& Segmentation** have been released, including [Grounding DINO](https://arxiv.org/abs/2303.05499)![Github stars](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg) and [OpenSeed](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/OpenSeed.svg). Check them out!
- \[2023/3\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) and [DN-DETR](https://arxiv.org/pdf/2203.01305) are selected as [the top 100 most cited AI papers for 2022](https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022), rank 38 and 53, respectively.
- \[2022/6\]: &nbsp;A series of works that pushes Transformer-based **Close-set Detection \& Segmentation** models to SOTA performance, including [Mask DINO](https://arxiv.org/pdf/2206.02777.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/MaskDINO.svg), [DINO](https://arxiv.org/abs/2203.03605)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DINO.svg), [DN-DETR](https://arxiv.org/pdf/2203.01305)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DN-DETR.svg), [DAB-DETR](https://arxiv.org/abs/2201.12329)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DAB-DETR.svg), have been released. 



# üìù Selected Works
Refer to my [google scholar](https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN) for the full list.

<!-- <div class='paper-box'>
<div class='paper-box-text' markdown="1"> -->
* **Visual In-Context Prompting**.  
**Feng Li**, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao.  
CVPR 2024.    
[[**Paper**]](https://arxiv.org/pdf/2307.04767.pdf)[[**Code**]](https://github.com/UX-Decoder/Semantic-SAM)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv?style=social">

* **SoM: Set-of-Mark Visual Prompting for GPT-4V**.  
Jianwei Yang\*, Hao Zhang\*, **Feng Li\***, Xueyan Zou\*, Chunyuan Li, Jianfeng Gao.                 
arxiv 2023.  
[[**Paper**]](https://arxiv.org/pdf/2310.11441.pdf)[[**Code**]](https://github.com/microsoft/SoM)![Github stars](https://img.shields.io/github/stars/microsoft/SoM.svg)

* **Semantic-SAM: Segment and Recognize Anything at Any Granularity**.  
**Feng Li\***, Hao Zhang\*, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao.                                   
ECCV 2024.    
[[**Paper**]](https://arxiv.org/pdf/2307.04767.pdf)[[**Code**]](https://github.com/UX-Decoder/Semantic-SAM)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Semantic-SAM.svg)

* **OpenSeeD: A Simple Framework for Open-Vocabulary Segmentation and Detection**.  
Hao Zhang\*, **Feng Li\***, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang.
ICCV 2023.  
[[**Paper**]](https://arxiv.org/abs/2303.08131)[[**Code**]](https://github.com/IDEA-Research/OpenSeeD)![Github stars](https://img.shields.io/github/stars/IDEA-Research/OpenSeed.svg)

* **SEEM: Segment Everything Everywhere All at Once**.  
Xueyan Zou\*, Jianwei Yang\*, Hao Zhang\*, **Feng Li\***, Linjie Li, Jianfeng Gao, Yong Jae Lee.   
NeurIPS 2023.  
[[**Paper**]](https://arxiv.org/pdf/2304.06718.pdf)[[**Code**]](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg)

* **Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**.  
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, **Feng Li**, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.   
ECCV 2024.  
[[**Paper**]](https://arxiv.org/abs/2303.05499)[[**Code**]](https://github.com/IDEA-Research/GroundingDINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg)

* **Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation**.  
**Feng Li**\*, Hao Zhang\*, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M. Ni, Heung-Yeung Shum.   
CVPR 2023.  **Rank 9th on CVPR 2023 Most Inflentical Papers**  
[[**Paper**]](https://arxiv.org/pdf/2206.02777.pdf)[[**Code**]](https://github.com/IDEACVR/MaskDINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/MaskDINO.svg)

* **DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection**.  
Hao Zhang\*, **Feng Li**\*, Shilong Liu\*, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum.   
ICLR 2023.  **Rank 2nd on ICLR 2023 Most Inflentical Papers**  
[[**Paper**]](https://arxiv.org/abs/2203.03605)[[**Code**]](https://github.com/IDEACVR/DINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DINO.svg) 
  
* **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**.   
**Feng Li**\*, Hao Zhang\*, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.   
CVPR 2022 | TPAMI 2023. **Oral** presentation.   
[[**Paper**]](https://arxiv.org/pdf/2203.01305)[[**Code**]](https://github.com/FengLi-ust/DN-DETR)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DN-DETR.svg)
 

_(* denotes equal contribution.)_
# üéñ Selected Awards
* Hong Kong Postgraduate Scholoarship, 2021
* Contemporary Undergraduate Mathematical Contest in Modeling(CUMCM), National first prize, 2019.

<!-- # üìñ Work experience
* March 2021 - Now: Research Assistant
  * Microsoft Research Asia, Beijing, China.
  * Duties included: 1. Design more powerful and simple object detection architecture based on the Transformer. 2. Understand NLP tasks such as NLI and exploit new paradigms to solve them more efficiently.
  * Advisor: Prof. [Jingdong Wang](https://jingdongwang2017.github.io/)

* August 2020 - Now: Research Assistant
  * University of Chinese Academy of Sciences, Beijing, China.
  * Duties included: 1. learning deep generative model for pedestrian generation. 2. cross-domain Re-ID from a causal view. 3. designing an efficient method to tackle problems in object detection and partial pedestrian re-identification.
  * Advisor: Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu)
  * Co-Advisors: Prof. [Zhang Zhang](https://scholar.google.com/citations?user=rnRNwEMAAAAJ&hl=en) and Prof. [Liang Wang](https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN)

* April 2018 ‚Äì July 2020: Research Assistant
  * South China University of Technology, Guangzhou, China.
  * Duties included: Incentive mechanism design for crowdsourcing platforms, edge computing
platforms, and federal learning platforms.
  * Advisor: Prof. Xinglin Zhang
 -->
<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<a href="http://s01.flagcounter.com/more/k4"><img src="https://s01.flagcounter.com/map/k4/size_s/txt_000000/border_CCCCCC/pageviews_0/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>

<a href="https://info.flagcounter.com/PUlW"><img src="https://s01.flagcounter.com/mini/PUlW/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>

