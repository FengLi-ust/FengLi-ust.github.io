---
permalink: /
title: About Me
excerpt: "Feng Li's Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a research scientist at Google DeepMind. Previously, I completed my PhD at [HKUST](https://hkust.edu.hk/) [CSE](https://cse.hkust.edu.hk/), co-supervised by Prof. [Heung-Yeung Shum](https://scholar.google.com/citations?user=9akH-n8AAAAJ&hl=zh-CN) and Prof. [Lionel M. Ni](https://scholar.google.com/citations?user=OzMYwDIAAAAJ&hl=zh-CN). Before that, I obtained my B.Eng. from South China University of Science and Technology. 

<!--
I have interned at [International Digital Economy Academy, Shenzhen](https://idea.edu.cn/) (advised by Prof. [Lei Zhang](https://www.leizhang.org/)), [Microsoft Research, Redmond](https://www.microsoft.com/en-us/research/group/deep-learning-group/) (advised by Dr. [Jianwei Yang](https://jwyang.github.io/) and Dr. [Chunyuan Li](https://chunyuan.li/)), [ByteDance](https://team.doubao.com/en/?view_from=homepage_tab), and [Meta AI (Facebook AI Research, Menlo Park)](https://research.facebook.com/). 
-->
I have interned at [International Digital Economy Academy, Shenzhen](https://idea.edu.cn/), [Microsoft Research, Redmond](https://www.microsoft.com/en-us/research/group/deep-learning-group/), [Meta AI (FAIR), Menlo Park](https://research.facebook.com/), and [ByteDance Seed, Shenzhen](https://team.doubao.com/en/?view_from=homepage_tab). 

<!--
**Looking for interns and students to work on unified understanding and generation models and world models**. Feel free to contact me if you are interested!
-->

üìåMy research focuses on multi-modal learning and fine-grained visual understanding. My previous work can be categorized into three main areas:
- Improve **multi-model LLMs** for visual understanding and generation, like [LLaVA-Next series](https://llava-vl.github.io/blog/)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social"> (i.e, [LLaVA-Interleave](https://arxiv.org/pdf/2407.07895) and [LLaVA-OneVision](https://arxiv.org/pdf/2408.03326)) and [BAGEL](https://github.com/bytedance-seed/BAGEL)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/bytedance-seed/BAGEL?style=social">.
- Enable more **promotable detection and grounding systems**, including
  - Visual in-context prompt, including [DINOv](https://arxiv.org/pdf/2311.13601.pdf)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv?style=social"> and [T-Rex/T-Rex2](https://arxiv.org/abs/2311.13596)![Github stars](https://img.shields.io/github/stars/IDEA-Research/T-Rex.svg).
  - Visual geometry prompt, including [SEEM](https://arxiv.org/pdf/2304.06718.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg) and [Semantic-SAM](https://arxiv.org/pdf/2307.04767.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Semantic-SAM.svg).
  - Text prompt, including [OpenSeed](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/OpenSeed.svg) and [Grounding DINO](https://arxiv.org/abs/2303.05499)![Github stars](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg).
- Push **close-set detection and segmentation** performance, including [Mask DINO](https://arxiv.org/pdf/2206.02777.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/MaskDINO.svg), [DINO](https://arxiv.org/abs/2203.03605)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DINO.svg), [DN-DETR](https://arxiv.org/pdf/2203.01305)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DN-DETR.svg), and [DAB-DETR](https://arxiv.org/abs/2201.12329)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DAB-DETR.svg).

<!--
I anticipate graduating in 2025 and am open to both academic and industrial research positions in North America and Asia. If you are interested, please feel free to contact me.
 I am always open to research discussions and collaborations. Feel free to get in touch! -->
<!-- 
**Research Interests** -->



‚úâÔ∏è Welcome to contact me for any discussion and cooperation!


# üî• News

- \[2025/5\]: &nbsp;[BAGEL](https://github.com/bytedance-seed/BAGEL) is out! An open-source unified model for visual understanding and generation, trained on large‚Äëscale interleaved multimodal data.
- \[2024/9\]: &nbsp;[Grounding DINO](https://arxiv.org/abs/2303.05499) is selected as the [most influential ECCV 2024 papers](https://www.paperdigest.org/2024/09/most-influential-eccv-papers-2024-09/).

- \[2024/7\]: &nbsp;[LLaVA-Interleave](https://arxiv.org/pdf/2407.07895) is out! We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM. Check out [blog](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/), [train data](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data), [LLaVA-Interleave Bench](https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench) and [code](https://github.com/LLaVA-VL/LLaVA-NeXT)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social"> to see new capabilities and improved performance! 
- \[2024/3\]: &nbsp;Check our recent works on **Visual Prompting** for detection and segmentation! A series of works including [DINOv](https://arxiv.org/pdf/2311.13601.pdf)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv?style=social">, [T-Rex/T-Rex2](https://arxiv.org/abs/2311.13596)![Github stars](https://img.shields.io/github/stars/IDEA-Research/T-Rex.svg) have been released.
- \[2023/9\]: &nbsp;[Mask DINO](https://arxiv.org/pdf/2206.02777.pdf) is selected as [the most influential CVPR 2023 papers](https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/).
- \[2023/7\]: &nbsp;Two works that focus on **Interactive Segmentation** have been released, including [SEEM](https://arxiv.org/pdf/2304.06718.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg) and [Semantic-SAM](https://arxiv.org/pdf/2307.04767.pdf)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Semantic-SAM.svg). Check them out!
- \[2023/4\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) and [DAB-DETR](https://arxiv.org/abs/2201.12329) are selected as [the most influential ICLR 2023 and ICLR 2022 papers](https://www.paperdigest.org/2023/09/most-influential-iclr-papers-2023-09/).
- \[2023/3\]: &nbsp;Two works on **Open-set Detection & Segmentation** have been released, including [Grounding DINO](https://arxiv.org/abs/2303.05499)![Github stars](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg) and [OpenSeed](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/OpenSeed.svg). Check them out!
- \[2023/3\]: &nbsp;[DINO](https://arxiv.org/abs/2203.0360) and [DN-DETR](https://arxiv.org/pdf/2203.01305) are selected as [the top 100 most cited AI papers for 2022](https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022), rank 38 and 53, respectively.
- \[2022/6\]: &nbsp;A series of works that push Transformer-based **Close-set Detection & Segmentation** models to SOTA performance, including [Mask DINO](https://arxiv.org/pdf/2206.02777.pdf)![Github stars](https://img.shields.io/github/stars/IDEA-Research/MaskDINO.svg), [DINO](https://arxiv.org/abs/2203.03605)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DINO.svg), [DN-DETR](https://arxiv.org/pdf/2203.01305)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DN-DETR.svg), [DAB-DETR](https://arxiv.org/abs/2201.12329)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DAB-DETR.svg), have been released. 



# üìù Selected Works
Refer to my [google scholar](https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN) for the full list.

<!-- <div class='paper-box'>
<div class='paper-box-text' markdown="1"> -->
* **BAGEL: Emerging Properties in Unified Multimodal Pretraining**.  
Chaorui Deng\*, Deyao Zhu\*, Kunchang Li\*, Chenhui Gou\*, **Feng Li**\*, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan\*.      
Arxiv, 2025.    
[[**Paper**]](https://arxiv.org/abs/2505.14683)[[**Website**]](https://bagel-ai.org/)[[**Code**]](https://github.com/ByteDance-Seed/Bagel)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/bytedance-seed/BAGEL?style=social">

* **LLaVA-OneVision: Easy Visual Task Transfer**.   
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, **Feng Li**, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li.   
TMLR, 2025.  
[[**Paper**]](https://arxiv.org/pdf/2408.03326)[[**Website**]](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/)[[**Code**]](https://github.com/LLaVA-VL/LLaVA-NeXT)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">

* **LLaVA-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models**.  
**Feng Li**\*, Renrui Zhang\*, Hao Zhang\*, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li  
ICLR 2025.    
[[**Paper**]](https://arxiv.org/pdf/2407.07895)[[**Blog**]](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/)[[**Code**]](https://github.com/LLaVA-VL/LLaVA-NeXT)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">

* **Visual In-Context Prompting**.  
**Feng Li**, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao.  
CVPR 2024.    
[[**Paper**]](https://arxiv.org/pdf/2307.04767.pdf)[[**Code**]](https://github.com/UX-Decoder/Semantic-SAM)<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv?style=social">

* **SoM: Set-of-Mark Visual Prompting for GPT-4V**.  
Jianwei Yang\*, Hao Zhang\*, **Feng Li\***, Xueyan Zou\*, Chunyuan Li, Jianfeng Gao.                 
arxiv 2023.  
[[**Paper**]](https://arxiv.org/pdf/2310.11441.pdf)[[**Code**]](https://github.com/microsoft/SoM)![Github stars](https://img.shields.io/github/stars/microsoft/SoM.svg)

* **Semantic-SAM: Segment and Recognize Anything at Any Granularity**.  
**Feng Li\***, Hao Zhang\*, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao.                                   
ECCV 2024.    
[[**Paper**]](https://arxiv.org/pdf/2307.04767.pdf)[[**Code**]](https://github.com/UX-Decoder/Semantic-SAM)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Semantic-SAM.svg)

* **OpenSeeD: A Simple Framework for Open-Vocabulary Segmentation and Detection**.  
Hao Zhang\*, **Feng Li\***, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang.    
ICCV 2023.  
[[**Paper**]](https://arxiv.org/abs/2303.08131)[[**Code**]](https://github.com/IDEA-Research/OpenSeeD)![Github stars](https://img.shields.io/github/stars/IDEA-Research/OpenSeed.svg)

* **SEEM: Segment Everything Everywhere All at Once**.  
Xueyan Zou\*, Jianwei Yang\*, Hao Zhang\*, **Feng Li\***, Linjie Li, Jianfeng Gao, Yong Jae Lee.   
NeurIPS 2023.  
[[**Paper**]](https://arxiv.org/pdf/2304.06718.pdf)[[**Code**]](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)![Github stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg)

* **Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**.  
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, **Feng Li**, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.   
ECCV 2024.  
[[**Paper**]](https://arxiv.org/abs/2303.05499)[[**Code**]](https://github.com/IDEA-Research/GroundingDINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO.svg)

* **Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation**.  
**Feng Li**\*, Hao Zhang\*, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M. Ni, Heung-Yeung Shum.   
CVPR 2023.  **Rank 9th on CVPR 2023 Most Inflentical Papers**  
[[**Paper**]](https://arxiv.org/pdf/2206.02777.pdf)[[**Code**]](https://github.com/IDEACVR/MaskDINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/MaskDINO.svg)

* **DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection**.  
Hao Zhang\*, **Feng Li**\*, Shilong Liu\*, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum.   
ICLR 2023.  **Rank 2nd on ICLR 2023 Most Inflentical Papers**  
[[**Paper**]](https://arxiv.org/abs/2203.03605)[[**Code**]](https://github.com/IDEACVR/DINO)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DINO.svg) 
  
* **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**.   
**Feng Li**\*, Hao Zhang\*, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.   
CVPR 2022 | TPAMI 2023. **Oral** presentation.   
[[**Paper**]](https://arxiv.org/pdf/2203.01305)[[**Code**]](https://github.com/FengLi-ust/DN-DETR)![Github stars](https://img.shields.io/github/stars/IDEA-Research/DN-DETR.svg)
 

_(* denotes equal contribution or core contributor.)_
# üéñ Selected Awards
* Hong Kong Postgraduate Scholoarship, 2021
* Contemporary Undergraduate Mathematical Contest in Modeling(CUMCM), National first prize, 2019.

<!-- # üìñ Work experience
* March 2021 - Now: Research Assistant
  * Microsoft Research Asia, Beijing, China.
  * Duties included: 1. Design more powerful and simple object detection architecture based on the Transformer. 2. Understand NLP tasks such as NLI and exploit new paradigms to solve them more efficiently.
  * Advisor: Prof. [Jingdong Wang](https://jingdongwang2017.github.io/)

* August 2020 - Now: Research Assistant
  * University of Chinese Academy of Sciences, Beijing, China.
  * Duties included: 1. learning deep generative model for pedestrian generation. 2. cross-domain Re-ID from a causal view. 3. designing an efficient method to tackle problems in object detection and partial pedestrian re-identification.
  * Advisor: Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu)
  * Co-Advisors: Prof. [Zhang Zhang](https://scholar.google.com/citations?user=rnRNwEMAAAAJ&hl=en) and Prof. [Liang Wang](https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN)

* April 2018 ‚Äì July 2020: Research Assistant
  * South China University of Technology, Guangzhou, China.
  * Duties included: Incentive mechanism design for crowdsourcing platforms, edge computing
platforms, and federal learning platforms.
  * Advisor: Prof. Xinglin Zhang
 -->
<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<a href="http://s01.flagcounter.com/more/k4"><img src="https://s01.flagcounter.com/map/k4/size_s/txt_000000/border_CCCCCC/pageviews_0/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>

<a href="https://info.flagcounter.com/PUlW"><img src="https://s01.flagcounter.com/mini/PUlW/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>

